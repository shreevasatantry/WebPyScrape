{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Shreevasa_201051005_BeautifulSoup.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreevasatantry/WebPyScrape/blob/main/Shreevasa_201051005_BeautifulSoup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3OAHRC_Ndzx"
      },
      "source": [
        "## Job search from indeed.com site"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuOoWgEUNdzy"
      },
      "source": [
        "* Importing the required libraries.\n",
        "* Here i have used BeautifulSoup library to scrape the indeed.com website."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNWk8OmUNdzz"
      },
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9c6zIt9Ndz0"
      },
      "source": [
        "# Taking the base job-url of indeed website.\n",
        "baseurl = 'https://in.indeed.com/{}-jobs'\n",
        "job = input('Search Jobs Here(Eg: python,java,html etc) :')\n",
        "# forming the proper Url using .format() operation with the help of user's job title\n",
        "url=baseurl.format(job)\n",
        "\n",
        "res = requests.get(url)\n",
        "soup = bs(res.content,'lxml') \n",
        "\n",
        "# Creation of empty list\n",
        "lst=[]\n",
        "# This for loop will fetch first 10 jobs from the web-page\n",
        "# And it will store the job-title,\n",
        "# company-name,location of the company and date in the respective variables.\n",
        "for i in range(10):\n",
        "    title=(soup.find_all('a',class_=['jobtitle'])[i].text.replace('\\n',''))\n",
        "    company=(soup.find_all('span',class_=['company'])[i].text.replace('\\n',''))\n",
        "    loc=(soup.find_all('span',class_=['location','accessible-contrast-color-location'])[i].text)\n",
        "    date=(soup.find_all('span',class_=['date'])[i].text)\n",
        "    dict={'Title':title,'Company':company,'Location':loc,'Date':date}\n",
        "    lst.append(dict)\n",
        "lst #list of dictionaries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ju_Xu4hNdz1"
      },
      "source": [
        "df = pd.DataFrame(lst)\n",
        "df #converting the list to pandas DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puf6JKH6Ndz1"
      },
      "source": [
        "#using seaborn library to plot the frequency plot from the scraped data\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(22, 8))\n",
        "option = input('Group and visualize based on (Title or Company or Location or Date) :')\n",
        "ax = sns.countplot(x=option,data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEbAYEnxNdz1"
      },
      "source": [
        "# Importing sqlite3 to dump the data from dataframe to the sql database\n",
        "import sqlite3\n",
        "conn = sqlite3.connect('JobDetails.db')\n",
        "c = conn.cursor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EblmSE6yNdz1"
      },
      "source": [
        "try:\n",
        "    c.execute('CREATE TABLE JOBS (title text, company text,location text,date text)')\n",
        "    conn.commit()\n",
        "except:\n",
        "    print('table Jobs already created!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXoPYGc9Ndz2"
      },
      "source": [
        "df.to_sql('JOBS', conn, if_exists='replace', index = False)\n",
        "c.execute('''  \n",
        "SELECT * FROM JOBS\n",
        "         ''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOoCHnb8Ndz2"
      },
      "source": [
        "for row in c.fetchall():\n",
        "    print (row) #printing the data from the database"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4B1Yn7qNdz2"
      },
      "source": [
        "* Converting pandas dataframe to csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tjC6qAhNdz2"
      },
      "source": [
        "df.to_csv('job_data.csv') #job_data.csv is created in the directory with title,company,location,date as feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7q4M34WN4N6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}